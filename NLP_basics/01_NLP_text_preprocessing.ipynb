{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/aT/46Ml3nW2fvlcqfZgG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/nlp_rag/blob/master/NLP_basics/01_NLP_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "\n",
        "![](https://devopedia.org/images/article/293/1027.1608556695.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "iIZOFCBcvzYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEFynr6ev2Iy",
        "outputId": "93ad2d7a-dc98-440f-b07b-5a0b5d8b0d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove special characters and punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 3. Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "    # 5. Stemming or Lemmatization (choose one)\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    #tokens = [stemmer.stem(w) for w in tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "\n",
        "    # 6. Remove numbers (if needed)\n",
        "    # tokens = [w for w in tokens if not w.isdigit()]\n",
        "\n",
        "    # 7. Join tokens back into a string\n",
        "    cleaned_text = \" \".join(tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "email_text = \"\"\"\n",
        "Hi Team,\n",
        "\n",
        "Just a quick reminder about our project update meeting tomorrow at 10:00 AM in the conference room. Please come prepared with your progress reports and any questions you might have. If you can't attend, let me know in advance.\n",
        "\n",
        "Looking forward to seeing everyone there!\n",
        "\n",
        "Best,\n",
        "John\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Message\")\n",
        "print(email_text)\n",
        "print(\"=\"*20)\n",
        "\n",
        "cleaned_text = clean_text(email_text)\n",
        "cleaned_text\n",
        "print(\"Cleaned Message\")\n",
        "print(\"=\"*20)\n",
        "print(cleaned_text)\n",
        "print(\"-\"*20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nfb80TGvzCy",
        "outputId": "fd450e17-e183-4e1a-8bef-7c4ff39ee8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Message\n",
            "\n",
            "Hi Team,\n",
            "\n",
            "Just a quick reminder about our project update meeting tomorrow at 10:00 AM in the conference room. Please come prepared with your progress reports and any questions you might have. If you can't attend, let me know in advance.\n",
            "\n",
            "Looking forward to seeing everyone there!\n",
            "\n",
            "Best,\n",
            "John\n",
            "\n",
            "====================\n",
            "Cleaned Message\n",
            "====================\n",
            "hi team quick reminder project update meeting tomorrow 1000 conference room please come prepared progress report question might cant attend let know advance looking forward seeing everyone best john\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and Lemmatization\n",
        "\n",
        "Stemming and Lemmatization are text normalization techniques used in Natural Language Processing (NLP) to reduce words to their base or root form.\n",
        "\n",
        "    Pros:\n",
        "\n",
        "    - Speed: Stemming is generally faster because it uses simple rules.\n",
        "    - Simplicity: Easy to implement and understand.\n",
        "    \n",
        "    Cons:\n",
        "\n",
        "    - Accuracy: Can be less accurate as it may produce non-existent words (e.g., \"studies\" becomes \"studi\").\n",
        "    - Context Ignorance: Does not consider the context or part of speech, leading to potential errors.\n",
        "\n",
        "**Lemmatization**\n",
        "\n",
        "Lemmatization reduces words to their base or dictionary form (lemma) by considering the context and part of speech. For example, \"running\" becomes \"run\" and \"better\" becomes \"good\".\n",
        "\n",
        "    Pros:\n",
        "\n",
        "    - Accuracy: More accurate as it produces valid words.\n",
        "    - Context Awareness: Considers the context and part of speech, leading to better results.\n",
        "    Cons:\n",
        "\n",
        "    - Speed: Slower compared to stemming because it involves looking up words in a dictionary.\n",
        "    - Complexity: More complex to implement and requires more computational resources.\n",
        "Why Are They Used?\n",
        "Both stemming and lemmatization are used to:\n",
        "\n",
        "Reduce Vocabulary Size: By converting words to their root form, the vocabulary size is reduced, making it easier to analyze and model the text.\n",
        "Improve Model Performance: Helps in improving the performance of NLP models by standardizing words, which can lead to better generalization.\n",
        "Enhance Text Analysis: Facilitates more accurate text analysis by grouping similar words together.\n"
      ],
      "metadata": {
        "id": "buFABTyqw81X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**\n",
        "\n",
        "there are different methods: Porter Stemming and Snowball Stemming methods"
      ],
      "metadata": {
        "id": "rU8JAFUR27Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This function implements the Bag of Words (BoW) model.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2JUrnRa1esV",
        "outputId": "0c974a86-dbd3-4dbc-8765-ee13649adc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'function', 'implement', 'the', 'bag', 'of', 'word', '(', 'bow', ')', 'model', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "gwwGXcL91fBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This function implements the Bag of Words (BoW) model.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORi1fnN7w52e",
        "outputId": "193bdbde-af1f-460b-c5c4-656287110bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'function', 'implement', 'the', 'Bag', 'of', 'Words', '(', 'BoW', ')', 'model', '.']\n"
          ]
        }
      ]
    }
  ]
}