{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWv5HxqapPdCuyqTv92sha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/nlp_rag/blob/master/NLP_basics/01_NLP_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "\n",
        "![](https://devopedia.org/images/article/293/1027.1608556695.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "iIZOFCBcvzYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEFynr6ev2Iy",
        "outputId": "93ad2d7a-dc98-440f-b07b-5a0b5d8b0d06"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove special characters and punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 3. Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "    # 5. Stemming or Lemmatization (choose one)\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    #tokens = [stemmer.stem(w) for w in tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "\n",
        "    # 6. Remove numbers (if needed)\n",
        "    # tokens = [w for w in tokens if not w.isdigit()]\n",
        "\n",
        "    # 7. Join tokens back into a string\n",
        "    cleaned_text = \" \".join(tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "email_text = \"\"\"\n",
        "Hi Team,\n",
        "\n",
        "Just a quick reminder about our project update meeting tomorrow at 10:00 AM in the conference room. Please come prepared with your progress reports and any questions you might have. If you can't attend, let me know in advance.\n",
        "\n",
        "Looking forward to seeing everyone there!\n",
        "\n",
        "Best,\n",
        "John\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Message\")\n",
        "print(email_text)\n",
        "print(\"=\"*20)\n",
        "\n",
        "cleaned_text = clean_text(email_text)\n",
        "cleaned_text\n",
        "print(\"Cleaned Message\")\n",
        "print(\"=\"*20)\n",
        "print(cleaned_text)\n",
        "print(\"-\"*20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nfb80TGvzCy",
        "outputId": "fd450e17-e183-4e1a-8bef-7c4ff39ee8c3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Message\n",
            "\n",
            "Hi Team,\n",
            "\n",
            "Just a quick reminder about our project update meeting tomorrow at 10:00 AM in the conference room. Please come prepared with your progress reports and any questions you might have. If you can't attend, let me know in advance.\n",
            "\n",
            "Looking forward to seeing everyone there!\n",
            "\n",
            "Best,\n",
            "John\n",
            "\n",
            "====================\n",
            "Cleaned Message\n",
            "====================\n",
            "hi team quick reminder project update meeting tomorrow 1000 conference room please come prepared progress report question might cant attend let know advance looking forward seeing everyone best john\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and Lemmatization\n",
        "\n",
        "Stemming and Lemmatization are text normalization techniques used in Natural Language Processing (NLP) to reduce words to their base or root form.\n",
        "\n",
        "    Pros:\n",
        "\n",
        "    - Speed: Stemming is generally faster because it uses simple rules.\n",
        "    - Simplicity: Easy to implement and understand.\n",
        "    \n",
        "    Cons:\n",
        "\n",
        "    - Accuracy: Can be less accurate as it may produce non-existent words (e.g., \"studies\" becomes \"studi\").\n",
        "    - Context Ignorance: Does not consider the context or part of speech, leading to potential errors.\n",
        "\n",
        "**Lemmatization**\n",
        "\n",
        "Lemmatization reduces words to their base or dictionary form (lemma) by considering the context and part of speech. For example, \"running\" becomes \"run\" and \"better\" becomes \"good\".\n",
        "\n",
        "    Pros:\n",
        "\n",
        "    - Accuracy: More accurate as it produces valid words.\n",
        "    - Context Awareness: Considers the context and part of speech, leading to better results.\n",
        "    Cons:\n",
        "\n",
        "    - Speed: Slower compared to stemming because it involves looking up words in a dictionary.\n",
        "    - Complexity: More complex to implement and requires more computational resources.\n",
        "Why Are They Used?\n",
        "Both stemming and lemmatization are used to:\n",
        "\n",
        "Reduce Vocabulary Size: By converting words to their root form, the vocabulary size is reduced, making it easier to analyze and model the text.\n",
        "Improve Model Performance: Helps in improving the performance of NLP models by standardizing words, which can lead to better generalization.\n",
        "Enhance Text Analysis: Facilitates more accurate text analysis by grouping similar words together.\n"
      ],
      "metadata": {
        "id": "buFABTyqw81X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**\n",
        "\n",
        "there are different methods: Porter Stemming and Snowball Stemming methods"
      ],
      "metadata": {
        "id": "rU8JAFUR27Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This function implements the Bag of Words (BoW) model.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2JUrnRa1esV",
        "outputId": "0c974a86-dbd3-4dbc-8765-ee13649adc12"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'function', 'implement', 'the', 'bag', 'of', 'word', '(', 'bow', ')', 'model', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "gwwGXcL91fBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This function implements the Bag of Words (BoW) model.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORi1fnN7w52e",
        "outputId": "193bdbde-af1f-460b-c5c4-656287110bc6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'function', 'implement', 'the', 'Bag', 'of', 'Words', '(', 'BoW', ')', 'model', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction\n",
        "\n",
        "\n",
        "Feature extraction in Natural Language Processing (NLP) involves converting text data into numerical representations that can be used by machine learning models. Here are some well-known techniques:\n",
        "\n",
        "1. **Bag of Words (BoW)**: This technique involves tokenizing the text and creating a vocabulary of all unique words. Each document is then represented as a vector indicating the presence or absence (or frequency) of these words.\n",
        "\n",
        "2. **Term Frequency-Inverse Document Frequency (TF-IDF)**: This method adjusts the frequency of words by how often they appear in all documents, giving more importance to words that are unique to a document.\n",
        "\n",
        "3. **N-grams**: This technique involves creating combinations of 'n' consecutive words or characters from the text, capturing the context and order of words.\n",
        "\n",
        "4. **Word Embeddings**: Techniques like Word2Vec, GloVe, and FastText create dense vector representations of words, capturing semantic relationships between them.\n",
        "\n",
        "5. **Doc2Vec**: An extension of Word2Vec, this technique generates vector representations for entire documents, preserving the context of words within the document.\n",
        "\n",
        "6. **Named Entity Recognition (NER)**: This method identifies and classifies key information (entities) in the text, such as names of people, organizations, locations, etc.\n",
        "\n",
        "7. **Part-of-Speech (POS) Tagging**: This involves labeling words with their respective parts of speech (nouns, verbs, adjectives, etc.), which can be useful for syntactic analysis.\n",
        "\n",
        "8. **Latent Dirichlet Allocation (LDA)**: A topic modeling technique that discovers abstract topics within a collection of documents.\n",
        "\n",
        "9. **Dependency Parsing**: This technique analyzes the grammatical structure of a sentence, identifying relationships between \"head\" words and words which modify those heads.\n",
        "\n",
        "10. **BERT** (Bidirectional Encoder Representations from Transformers): A state-of-the-art technique that uses transformers to generate context-aware embeddings for words in a sentence12.\n",
        "\n",
        "These techniques are fundamental in various NLP tasks such as text classification, sentiment analysis, and information retrieval"
      ],
      "metadata": {
        "id": "ZLqyhQmw3Zn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words"
      ],
      "metadata": {
        "id": "Dh32-wlQ2vFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU4mKTL7qD3Q",
        "outputId": "52bcaf6a-bad4-4474-cdbd-46ebc5d94dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'feature': 2, 'extraction': 2, 'in': 1, 'natural': 1, 'language': 1, 'processing': 1, 'nlp': 1, 'involves': 1, 'converting': 1, 'text': 1, 'data': 1, 'into': 1, 'numerical': 1, 'representations': 1, 'that': 1, 'can': 1, 'be': 1, 'used': 1, 'by': 1, 'machine': 1, 'learning': 1, 'models': 1}\n",
            "Email 1: Spam score = 3\n",
            "Email 2: Spam score = 0\n",
            "Email 3: Spam score = 2\n",
            "Email 4: Spam score = 0\n"
          ]
        }
      ],
      "source": [
        "# prompt: Write an example for Bag of Words (BoW) showing its real-life application ?\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def bag_of_words(text):\n",
        "\n",
        "  # Preprocessing: Convert to lowercase and remove punctuation\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  # Tokenization: Split the text into words\n",
        "  words = text.split()\n",
        "\n",
        "  # Count word frequencies\n",
        "  word_counts = Counter(words)\n",
        "\n",
        "  return dict(word_counts)\n",
        "\n",
        "# Example Usage\n",
        "text = \"\"\"\n",
        "# ## Feature extraction\n",
        "Feature extraction in Natural Language Processing (NLP) involves converting text data into numerical representations that can be used by machine learning models.\n",
        "\"\"\"\n",
        "\n",
        "bow = bag_of_words(text)\n",
        "print(bow)\n",
        "\n",
        "\n",
        "# Real-life Application: Spam Detection\n",
        "\n",
        "# Assume we have a list of emails, some spam and some not spam\n",
        "emails = [\n",
        "  \"Free money! Click here now!\", # Spam\n",
        "  \"Meeting reminder: Project X discussion\", # Not Spam\n",
        "  \"Congratulations! You won a prize!\", # Spam\n",
        "  \"Check out our latest product updates\" # Not Spam\n",
        "]\n",
        "\n",
        "# Create BoW representations for each email\n",
        "email_bows = [bag_of_words(email) for email in emails]\n",
        "\n",
        "\n",
        "# Spam detection logic: We can use the BoW to detect spam by finding words frequently occurring in spam emails.\n",
        "spam_keywords = [\"free\", \"money\", \"prize\", \"click\", \"won\"]\n",
        "for i, bow in enumerate(email_bows):\n",
        "    spam_score = 0\n",
        "    for word in spam_keywords:\n",
        "        spam_score += bow.get(word, 0) # Check if the word exists in BoW, if not default to 0\n",
        "    print(f\"Email {i+1}: Spam score = {spam_score}\")\n",
        "\n",
        "# Based on the spam score, we can classify the email as spam or not spam using some threshold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4yiOiuYtuNX",
        "outputId": "05748d36-180f-4114-f195-1074316a52a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'check': 1, 'out': 1, 'our': 1, 'latest': 1, 'product': 1, 'updates': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "# ## Feature extraction\n",
        "Feature extraction in Natural Language Processing (NLP) involves converting text data into numerical representations that can be used by machine learning models.\n",
        "\"\"\"\n",
        "\n",
        "## Clean the text:\n",
        "alpha_numeric = r\"[^a-zA-Z0-9]\"\n",
        "clean_text = re.sub(alpha_numeric, \" \", text).strip()\n",
        "\n",
        "words = clean_text.split()\n",
        "bag={}\n",
        "for word in words:\n",
        "    word = word.lower().strip()\n",
        "    if word in bag.keys():\n",
        "        bag[word] += 1\n",
        "    else:\n",
        "        bag[word] = 1\n",
        "print(bag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB1gDCM6tadL",
        "outputId": "076eaee6-93bb-423b-bf0f-8ef0a4f19ed6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'feature': 2, 'extraction': 2, 'in': 1, 'natural': 1, 'language': 1, 'processing': 1, 'nlp': 1, 'involves': 1, 'converting': 1, 'text': 1, 'data': 1, 'into': 1, 'numerical': 1, 'representations': 1, 'that': 1, 'can': 1, 'be': 1, 'used': 1, 'by': 1, 'machine': 1, 'learning': 1, 'models': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "- Representation: TF-IDF adjusts the word counts by considering the importance of each word in the context of the entire corpus.\n",
        "- Term Frequency (TF): Measures how frequently a word appears in a document.\n",
        "- Inverse Document Frequency (IDF): Measures how important a word is by considering how common or rare it is across all documents.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/376247075/figure/fig2/AS:11431281209841725@1701888441866/TF-IDFTerm-Frequency-Inverse-Document-Frequency_W640.jpg\" height=200/>"
      ],
      "metadata": {
        "id": "AjGAUV1l7pS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step.1 Load the documents"
      ],
      "metadata": {
        "id": "ok0bboZa9Al6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents\n",
        "documents = [\n",
        "    \"this is the first document\",\n",
        "    \"this document is the second document\",\n",
        "    \"and this is the third one\",\n",
        "    \"is this the first document\"\n",
        "]"
      ],
      "metadata": {
        "id": "rzW0xN_-9Cto"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Split the documents\n",
        "\n",
        "List down all the words in all the documents. Assuming that we have already preprocessed the text"
      ],
      "metadata": {
        "id": "LJrdbjsd9DfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set()\n",
        "for doc in documents:\n",
        "    vocabulary.update(doc.split())\n",
        "\n",
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIMZRmwYAP4d",
        "outputId": "ecc2775b-4d68-411b-a33f-bfe0eaaad4eb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Calculate the term frequency\n",
        "\n",
        "Calculate the frequency of words occuring for each doc separately"
      ],
      "metadata": {
        "id": "Hm5DHMEa-gRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(sentence):\n",
        "    word_freq = {}\n",
        "    for word in sentence.split(\" \"):\n",
        "        if word in word_freq.keys():\n",
        "            word_freq[word] +=1\n",
        "        else:\n",
        "            word_freq[word] =1\n",
        "\n",
        "    return word_freq\n",
        "\n",
        "\n",
        "def calculate_tf(word_freq):\n",
        "    arr = np.array(list(word_freq.values()), dtype=np.float16)\n",
        "    return arr/len(word_freq.keys())\n",
        "\n",
        "s = \"calculate the frequency of words occuring for each doc separately\"\n",
        "word_freq=  word_count(s)\n",
        "print(word_freq)\n",
        "\n",
        "tf = calculate_tf(word_freq)\n",
        "print(tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2z0ukUbBjFB",
        "outputId": "cc90ed63-344a-4104-96a4-e9cd1fe67d16"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'calculate': 1, 'the': 1, 'frequency': 1, 'of': 1, 'words': 1, 'occuring': 1, 'for': 1, 'each': 1, 'doc': 1, 'separately': 1}\n",
            "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf = {}\n",
        "for i,doc in enumerate(documents):\n",
        "\n",
        "    word_freq = word_count(doc)\n",
        "    tf_values = calculate_tf(word_freq)\n",
        "    tf[i] = tf_values\n",
        "\n",
        "print(tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RxuzO4RAnEU",
        "outputId": "917294fd-d6c8-4cff-98b3-faa90543a988"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: array([0.2, 0.2, 0.2, 0.2, 0.2], dtype=float16), 1: array([0.2, 0.4, 0.2, 0.2, 0.2], dtype=float16), 2: array([0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666], dtype=float16), 3: array([0.2, 0.2, 0.2, 0.2, 0.2], dtype=float16)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4. Calculate Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "5zkFzfj8-tZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_word_freq = {}\n",
        "for i,doc in enumerate(documents):\n",
        "    word_freq = word_count(doc)\n",
        "    doc_word_freq[i] = word_freq\n",
        "\n",
        "doc_word_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak7Cx7KDEH9k",
        "outputId": "44842da0-c542-4cb6-f0da-a3cbd9afb658"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'this': 1, 'is': 1, 'the': 1, 'first': 1, 'document': 1},\n",
              " 1: {'this': 1, 'document': 2, 'is': 1, 'the': 1, 'second': 1},\n",
              " 2: {'and': 1, 'this': 1, 'is': 1, 'the': 1, 'third': 1, 'one': 1},\n",
              " 3: {'is': 1, 'this': 1, 'the': 1, 'first': 1, 'document': 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf = {}\n",
        "tolerance = 1 ## To avoid division by zero error\n",
        "for word in vocabulary:\n",
        "    doc_count = 0\n",
        "    for doc in documents:\n",
        "        if word in doc:\n",
        "            doc_count+=1\n",
        "\n",
        "    idf_val = np.log(len(documents)/(doc_count+tolerance))\n",
        "    idf[word] = idf_val\n",
        "\n",
        "idf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXEJDsYVD6_I",
        "outputId": "2f52b69b-44df-4124-d419-51d4ac2b7394"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': -0.2231435513142097,\n",
              " 'third': 0.6931471805599453,\n",
              " 'is': -0.2231435513142097,\n",
              " 'the': -0.2231435513142097,\n",
              " 'one': 0.6931471805599453,\n",
              " 'second': 0.6931471805599453,\n",
              " 'and': 0.6931471805599453,\n",
              " 'first': 0.28768207245178085,\n",
              " 'document': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf = {}\n",
        "for word in vocabulary:\n",
        "    doc_count = 0\n",
        "    for i in range(len(documents)):\n",
        "        if word in documents[i].split():\n",
        "            doc_count += 1\n",
        "    idf[word] = 1 + (len(documents) / doc_count)  # Adding 1 to avoid division by zero\n",
        "\n",
        "idf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCbKZcAD-yHX",
        "outputId": "81c02fd2-a2e5-48ed-8b68-7f55687511c9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 2.0,\n",
              " 'third': 5.0,\n",
              " 'is': 2.0,\n",
              " 'the': 2.0,\n",
              " 'one': 5.0,\n",
              " 'second': 5.0,\n",
              " 'and': 5.0,\n",
              " 'first': 3.0,\n",
              " 'document': 2.333333333333333}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5. Calculate Tf-IDF\n",
        "\n"
      ],
      "metadata": {
        "id": "UFx5mP1R-4o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TF-IDF\n",
        "tfidf = {}\n",
        "for i in range(len(documents)):\n",
        "  tfidf[i] = {}\n",
        "  for word in vocabulary:\n",
        "    tfidf[i][word] = (tf[i].get(word, 0) / sum(tf[i].values())) * idf[word]\n",
        "\n",
        "tfidf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPYGnfIz--IS",
        "outputId": "5213c5d0-106d-40ff-de4c-08da781a21d5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'this': 0.4,\n",
              "  'third': 0.0,\n",
              "  'is': 0.4,\n",
              "  'the': 0.4,\n",
              "  'one': 0.0,\n",
              "  'second': 0.0,\n",
              "  'and': 0.0,\n",
              "  'first': 0.6000000000000001,\n",
              "  'document': 0.4666666666666666},\n",
              " 1: {'this': 0.3333333333333333,\n",
              "  'third': 0.0,\n",
              "  'is': 0.3333333333333333,\n",
              "  'the': 0.3333333333333333,\n",
              "  'one': 0.0,\n",
              "  'second': 0.8333333333333333,\n",
              "  'and': 0.0,\n",
              "  'first': 0.0,\n",
              "  'document': 0.7777777777777777},\n",
              " 2: {'this': 0.3333333333333333,\n",
              "  'third': 0.8333333333333333,\n",
              "  'is': 0.3333333333333333,\n",
              "  'the': 0.3333333333333333,\n",
              "  'one': 0.8333333333333333,\n",
              "  'second': 0.0,\n",
              "  'and': 0.8333333333333333,\n",
              "  'first': 0.0,\n",
              "  'document': 0.0},\n",
              " 3: {'this': 0.4,\n",
              "  'third': 0.0,\n",
              "  'is': 0.4,\n",
              "  'the': 0.4,\n",
              "  'one': 0.0,\n",
              "  'second': 0.0,\n",
              "  'and': 0.0,\n",
              "  'first': 0.6000000000000001,\n",
              "  'document': 0.4666666666666666}}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the TF-IDF matrix\n",
        "for doc_id, tfidf_scores in tfidf.items():\n",
        "    print(f\"Document {doc_id}:\")\n",
        "    for word, score in tfidf_scores.items():\n",
        "        print(f\"  {word}: {score:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwuiAlvW8y6T",
        "outputId": "10543508-e1e3-451f-c5b0-f8783e485b9b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0:\n",
            "  this: 0.4000\n",
            "  third: 0.0000\n",
            "  is: 0.4000\n",
            "  the: 0.4000\n",
            "  one: 0.0000\n",
            "  second: 0.0000\n",
            "  and: 0.0000\n",
            "  first: 0.6000\n",
            "  document: 0.4667\n",
            "\n",
            "Document 1:\n",
            "  this: 0.3333\n",
            "  third: 0.0000\n",
            "  is: 0.3333\n",
            "  the: 0.3333\n",
            "  one: 0.0000\n",
            "  second: 0.8333\n",
            "  and: 0.0000\n",
            "  first: 0.0000\n",
            "  document: 0.7778\n",
            "\n",
            "Document 2:\n",
            "  this: 0.3333\n",
            "  third: 0.8333\n",
            "  is: 0.3333\n",
            "  the: 0.3333\n",
            "  one: 0.8333\n",
            "  second: 0.0000\n",
            "  and: 0.8333\n",
            "  first: 0.0000\n",
            "  document: 0.0000\n",
            "\n",
            "Document 3:\n",
            "  this: 0.4000\n",
            "  third: 0.0000\n",
            "  is: 0.4000\n",
            "  the: 0.4000\n",
            "  one: 0.0000\n",
            "  second: 0.0000\n",
            "  and: 0.0000\n",
            "  first: 0.6000\n",
            "  document: 0.4667\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implemeting with Sklearn library\n",
        "\n",
        "- TfidVectorizer"
      ],
      "metadata": {
        "id": "fk6tFD-AGLMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the sentences\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "## Vectorize the training data\n",
        "train_X = vectorizer.transform(documents)\n",
        "print(\"TF-IDF of all the documents is:\\n\", train_X.toarray())\n",
        "\n",
        "# Vectorize a new sentence\n",
        "\n",
        "new_X = vectorizer.transform([documents[0]])\n",
        "\n",
        "# Convert to array and print\n",
        "print(\"TF-IDF Vectorized form:\", new_X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etWp0bnX_q3G",
        "outputId": "e80f74bb-cbd5-488c-c71b-de8d80a78429"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "TF-IDF of all the documents is:\n",
            " [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "TF-IDF Vectorized form: [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Analysis/application\n",
        "\n",
        "Understanding the Vectors\n",
        "Each document is represented as a vector where each element corresponds to the TF-IDF score of a word in the vocabulary. For example:\n",
        "\n",
        "- Document 0 vector: `[0.4000, 0.0000, 0.4000, 0.4000, 0.0000, 0.0000, 0.0000, 0.6000, 0.4667]`\n",
        "- Document 1 vector: `[0.3333, 0.0000, 0.3333, 0.3333, 0.0000, 0.8333, 0.0000, 0.0000, 0.7778]`\n",
        "\n",
        "Next Steps\n",
        "1. Model Training: Use these vectors as input features to train machine learning models for tasks like text classification, clustering, or sentiment analysis.\n",
        "\n",
        "2. Similarity Measurement: Calculate the similarity between documents using metrics like cosine similarity, which can be useful for information retrieval or document clustering.\n",
        "\n",
        "3. Visualization: Visualize the document vectors using techniques like PCA (Principal Component Analysis) or t-SNE to understand the distribution of documents in the feature space"
      ],
      "metadata": {
        "id": "pk-W-NCg_ciz"
      }
    }
  ]
}