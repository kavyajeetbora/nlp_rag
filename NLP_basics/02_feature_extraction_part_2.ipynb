{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcko7NtZwldL1Vx9SZ0FkK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/nlp_rag/blob/master/NLP_basics/02_feature_extraction_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction and Vectorizing Methods - Part 2"
      ],
      "metadata": {
        "id": "ZLqyhQmw3Zn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "6VNiv2NpI9iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Grams\n",
        "\n",
        "N-grams are contiguous sequences of 'n' items from a given sample of text or speech. These items can be words, characters, or symbols, depending on the application. N-grams are widely used in various NLP tasks to capture the context and structure of the text.\n",
        "\n",
        "1. Types of N-grams\n",
        "- Unigram (n=1): Single words. Example: \"This is a test\" → [\"This\", \"is\", \"a\", \"test\"]\n",
        "- Bigram (n=2): Pairs of consecutive words. Example: \"This is a test\" → [\"This is\", \"is a\", \"a test\"]\n",
        "- Trigram (n=3): Triplets of consecutive words. Example: \"This is a test\" → [\"This is a\", \"is a test\"]\n",
        "\n",
        "2. Applications of N-grams\n",
        "- Language Modeling: Predicting the next word in a sequence based on the previous 'n' words.\n",
        "- Text Classification: Using N-grams as features to classify documents.\n",
        "Spelling Correction: Identifying and correcting misspelled words by analyzing N-gram patterns.\n",
        "- Machine Translation: Translating text by considering N-gram sequences to maintain context.\n",
        "- Text Mining: Extracting meaningful patterns and insights from text data."
      ],
      "metadata": {
        "id": "1K2PR2ZoJ8h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a test\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = text.split()\n",
        "\n",
        "# Generate bigrams (n=2)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)\n",
        "\n",
        "# Generate trigrams (n=3)\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(\"Trigrams:\", trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4zUec9jJwn3",
        "outputId": "2331006b-0836-4aac-cc0f-c5220eb643ae"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('This', 'is'), ('is', 'a'), ('a', 'test')]\n",
            "Trigrams: [('This', 'is', 'a'), ('is', 'a', 'test')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-grams, such as bigrams and trigrams, are used in various NLP tasks to capture the context and relationships between words. Here are some common applications:\n",
        "\n",
        "**Applications of N-grams**\n",
        "1. Language Modeling: N-grams help in predicting the next word in a sequence. For example, in a bigram model, the probability of a word depends on the previous word. This is useful in applications like text generation and autocomplete.\n",
        "\n",
        "2. Text Classification: N-grams can be used as features to classify documents. For instance, in spam detection, certain bigrams or trigrams might be more common in spam emails than in legitimate ones.\n",
        "\n",
        "3. Sentiment Analysis: N-grams capture phrases that convey sentiment. For example, bigrams like \"not good\" or \"very happy\" can provide more context than individual words.\n",
        "\n",
        "4. Spelling Correction: N-grams help in identifying and correcting misspelled words by analyzing the context in which they appear. For example, if \"hte\" appears frequently before \"cat\", it can be corrected to \"the\".\n",
        "\n",
        "5. Machine Translation: N-grams are used to maintain the context and fluency of translations. They help in ensuring that translated phrases make sense in the target language.\n",
        "\n",
        "6. Information Retrieval: Search engines use N-grams to improve the relevance of search results. For example, bigrams and trigrams can help in understanding user queries better."
      ],
      "metadata": {
        "id": "2ysi18ocKmBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample data\n",
        "texts = [\"This is a place\", \"Which place do you stay?\", \"This is a test\", \"Stay in this place\"]\n",
        "labels = [\"statement\", \"question\", \"statement\", \"statement\"]\n",
        "\n",
        "# Create a CountVectorizer with bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Create a pipeline with the vectorizer and a classifier\n",
        "model = make_pipeline(vectorizer, MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model.fit(texts, labels)\n",
        "\n",
        "# Predict the label of a new text\n",
        "new_text = [\"Where do you stay?\"]\n",
        "predicted_label = model.predict(new_text)\n",
        "print(\"Predicted label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmdooIBxLABU",
        "outputId": "fad6730f-4c6a-401a-9337-9b2d5f86fbeb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: ['question']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Reference and candidate translations\n",
        "reference = [['this', 'is', 'a', 'test']]\n",
        "candidate = ['this', 'was', 'a', 'test']\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu(reference, candidate)\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "871VurU6LtfW",
        "outputId": "519d09c2-1d0a-4383-81e4-61e7fde218cf"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 1.0547686614863434e-154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings\n",
        "\n",
        "Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space. These vectors capture semantic meanings and relationships between words, making them a powerful tool in NLP.\n",
        "\n",
        "Key Concepts\n",
        "1. Dense Vectors: Unlike traditional methods like Bag of Words (BoW) or TF-IDF, which create sparse vectors, word embeddings create dense vectors where each dimension captures some aspect of the word's meaning.\n",
        "\n",
        "2. Contextual Similarity: Words that are similar in meaning are placed closer together in the vector space. For example, \"king\" and \"queen\" would have similar vectors.\n",
        "\n",
        "3. Dimensionality Reduction: Word embeddings reduce the dimensionality of text data, making it more computationally efficient while preserving semantic relationships."
      ],
      "metadata": {
        "id": "91OXdb-9Q2UA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec\n",
        "\n",
        "Word2Vec: Developed by Google, it uses neural networks to learn word associations from a large corpus of text. It has two models: Continuous Bag of Words (CBOW) and Skip-gram."
      ],
      "metadata": {
        "id": "hW0bxA7eQ8xw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lfh_cFfVQ_wa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}