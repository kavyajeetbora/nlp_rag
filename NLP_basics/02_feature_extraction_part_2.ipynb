{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/5x2IijpKPe/MMu/1EYtq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/nlp_rag/blob/master/NLP_basics/02_feature_extraction_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction and Vectorizing Methods - Part 2"
      ],
      "metadata": {
        "id": "ZLqyhQmw3Zn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "6VNiv2NpI9iG",
        "outputId": "173fad25-0aea-4c93-fc68-d0bb1e6a3118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Grams\n",
        "\n",
        "N-grams are contiguous sequences of 'n' items from a given sample of text or speech. These items can be words, characters, or symbols, depending on the application. N-grams are widely used in various NLP tasks to capture the context and structure of the text.\n",
        "\n",
        "1. Types of N-grams\n",
        "- Unigram (n=1): Single words. Example: \"This is a test\" → [\"This\", \"is\", \"a\", \"test\"]\n",
        "- Bigram (n=2): Pairs of consecutive words. Example: \"This is a test\" → [\"This is\", \"is a\", \"a test\"]\n",
        "- Trigram (n=3): Triplets of consecutive words. Example: \"This is a test\" → [\"This is a\", \"is a test\"]\n",
        "\n",
        "2. Applications of N-grams\n",
        "- Language Modeling: Predicting the next word in a sequence based on the previous 'n' words.\n",
        "- Text Classification: Using N-grams as features to classify documents.\n",
        "Spelling Correction: Identifying and correcting misspelled words by analyzing N-gram patterns.\n",
        "- Machine Translation: Translating text by considering N-gram sequences to maintain context.\n",
        "- Text Mining: Extracting meaningful patterns and insights from text data."
      ],
      "metadata": {
        "id": "1K2PR2ZoJ8h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a test\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = text.split()\n",
        "\n",
        "# Generate bigrams (n=2)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)\n",
        "\n",
        "# Generate trigrams (n=3)\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(\"Trigrams:\", trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4zUec9jJwn3",
        "outputId": "228fc4a9-97a2-459b-a75d-70373144d970"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('This', 'is'), ('is', 'a'), ('a', 'test')]\n",
            "Trigrams: [('This', 'is', 'a'), ('is', 'a', 'test')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-grams, such as bigrams and trigrams, are used in various NLP tasks to capture the context and relationships between words. Here are some common applications:\n",
        "\n",
        "**Applications of N-grams**\n",
        "1. Language Modeling: N-grams help in predicting the next word in a sequence. For example, in a bigram model, the probability of a word depends on the previous word. This is useful in applications like text generation and autocomplete.\n",
        "\n",
        "2. Text Classification: N-grams can be used as features to classify documents. For instance, in spam detection, certain bigrams or trigrams might be more common in spam emails than in legitimate ones.\n",
        "\n",
        "3. Sentiment Analysis: N-grams capture phrases that convey sentiment. For example, bigrams like \"not good\" or \"very happy\" can provide more context than individual words.\n",
        "\n",
        "4. Spelling Correction: N-grams help in identifying and correcting misspelled words by analyzing the context in which they appear. For example, if \"hte\" appears frequently before \"cat\", it can be corrected to \"the\".\n",
        "\n",
        "5. Machine Translation: N-grams are used to maintain the context and fluency of translations. They help in ensuring that translated phrases make sense in the target language.\n",
        "\n",
        "6. Information Retrieval: Search engines use N-grams to improve the relevance of search results. For example, bigrams and trigrams can help in understanding user queries better."
      ],
      "metadata": {
        "id": "2ysi18ocKmBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample data\n",
        "texts = [\"This is a place\", \"Which place do you stay?\", \"This is a test\", \"Stay in this place\"]\n",
        "labels = [\"statement\", \"question\", \"statement\", \"statement\"]\n",
        "\n",
        "# Create a CountVectorizer with bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Create a pipeline with the vectorizer and a classifier\n",
        "model = make_pipeline(vectorizer, MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model.fit(texts, labels)\n",
        "\n",
        "# Predict the label of a new text\n",
        "new_text = [\"Where do you stay?\"]\n",
        "predicted_label = model.predict(new_text)\n",
        "print(\"Predicted label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmdooIBxLABU",
        "outputId": "850d4056-cb23-4eca-c131-afc23f11e5c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: ['question']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Reference and candidate translations\n",
        "reference = [['this', 'is', 'a', 'test']]\n",
        "candidate = ['this', 'was', 'a', 'test']\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu(reference, candidate)\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "871VurU6LtfW",
        "outputId": "055366f2-0e01-4f69-9a59-5898c5386694"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 1.0547686614863434e-154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings\n",
        "\n",
        "Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space. These vectors capture semantic meanings and relationships between words, making them a powerful tool in NLP.\n",
        "\n",
        "Key Concepts\n",
        "1. Dense Vectors: Unlike traditional methods like Bag of Words (BoW) or TF-IDF, which create sparse vectors, word embeddings create dense vectors where each dimension captures some aspect of the word's meaning.\n",
        "\n",
        "2. Contextual Similarity: Words that are similar in meaning are placed closer together in the vector space. For example, \"king\" and \"queen\" would have similar vectors.\n",
        "\n",
        "3. Dimensionality Reduction: Word embeddings reduce the dimensionality of text data, making it more computationally efficient while preserving semantic relationships."
      ],
      "metadata": {
        "id": "91OXdb-9Q2UA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec\n",
        "\n",
        "Word2Vec: Developed by Google, it uses neural networks to learn word associations from a large corpus of text. It has two models: Continuous Bag of Words (CBOW) and Skip-gram.\n",
        "\n",
        "Word2Vec: Developed by Google, it uses neural networks to learn word associations from a large corpus of text. It has two models:\n",
        "\n",
        "1. Continuous Bag of Words (CBOW): Predicts the target word from its surrounding context words. It is faster and works well with smaller datasets.\n",
        "2. Skip-gram: Predicts the surrounding context words from a target word. It is better at capturing semantic relationships and handling rare words, but is computationally more expensive.\n",
        "\n",
        "Word2Vec creates dense vector representations of words, capturing their meanings and relationships. These vectors are useful in various NLP tasks like text classification, sentiment analysis, and machine translation."
      ],
      "metadata": {
        "id": "hW0bxA7eQ8xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example usage\n",
        "sentences = [[\"this\", \"is\", \"a\", \"sample\"], [\"we\", \"are\", \"learning\", \"word2vec\"]]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "vector = model.wv['sample']\n",
        "print(vector)\n",
        "print(vector.shape)"
      ],
      "metadata": {
        "id": "Lfh_cFfVQ_wa",
        "outputId": "ccf1c58b-20a5-4afb-d33a-6cf3774fd00f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
            "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
            " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
            "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
            " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
            "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
            " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
            " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
            "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
            " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
            "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
            "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
            " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
            "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
            " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
            "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
            "  0.00620989  0.00481889  0.00078719  0.00301345]\n",
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GloVe\n",
        "\n",
        "GloVe (Global Vectors for Word Representation): Developed by Stanford, it combines the advantages of both global matrix factorization and local context window methods."
      ],
      "metadata": {
        "id": "uX1cFSLQVsiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sentence_glove(sentence, glove_vectors):\n",
        "\n",
        "    vectors = []\n",
        "    for word in sentence:\n",
        "        if word in glove_vectors:\n",
        "            vectors.append(glove_vectors[word])\n",
        "\n",
        "    if vectors:\n",
        "      return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "\n",
        "# Example usage (assuming you have loaded GloVe vectors into 'glove_vectors'):\n",
        "# Load pre-trained GloVe vectors (you'll need to download these separately)\n",
        "glove_vectors = {} # Initialize an empty dictionary\n",
        "\n",
        "# Example: Add some dummy vectors to the dictionary\n",
        "glove_vectors['the'] = np.array([0.1, 0.2, 0.3])\n",
        "glove_vectors['quick'] = np.array([0.4, 0.5, 0.6])\n",
        "glove_vectors['brown'] = np.array([0.7, 0.8, 0.9])\n",
        "glove_vectors['fox'] = np.array([1.0, 1.1, 1.2])\n",
        "\n",
        "\n",
        "sentence = [\"the\", \"quick\", \"brown\", \"fox\"]\n",
        "sentence_vector = vectorize_sentence_glove(sentence, glove_vectors)\n",
        "\n",
        "if sentence_vector is not None:\n",
        "    print(\"Sentence vector:\", sentence_vector)\n",
        "    print(\"Shape:\", sentence_vector.shape)\n",
        "else:\n",
        "    print(\"No words in the sentence were found in the GloVe vocabulary.\")"
      ],
      "metadata": {
        "id": "G7TGlvQrV1OX",
        "outputId": "2ae17ca7-24a2-435f-c911-418a82f59b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence vector: [0.55 0.65 0.75]\n",
            "Shape: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load the GloVe model\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")\n"
      ],
      "metadata": {
        "id": "Svgi97NoVmoe",
        "outputId": "0eff72ae-1831-4f76-b977-89297190730c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sentence(sentence, model):\n",
        "    words = sentence.split()\n",
        "    word_vectors = [model[word] for word in words if word in model]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)"
      ],
      "metadata": {
        "id": "CrguWpKXYu7K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GloVe (Global Vectors for Word Representation), each word is assigned a constant embedding. This means that every word has a fixed vector representation in the embedding space, which is determined based on the global word-word co-occurrence statistics from a large corpus:"
      ],
      "metadata": {
        "id": "wtioq9Nobg0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove_model['duplicate'].shape)"
      ],
      "metadata": {
        "id": "GB4fSUqXbY8V",
        "outputId": "e54a2a66-c2d1-47c6-f198-883175bc3477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentence\n",
        "sentence = \"This is an example sentence for vectorization.\"\n",
        "\n",
        "# Vectorize the sentence\n",
        "sentence_vector = vectorize_sentence(sentence, glove_model)\n",
        "\n",
        "print(f\"Vector representation of the sentence: {sentence_vector.shape}\")"
      ],
      "metadata": {
        "id": "qYuakqK5YszC",
        "outputId": "c4d9f431-e867-4dd6-e2a8-021c322fcf71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of the sentence: (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Math with word  vectors"
      ],
      "metadata": {
        "id": "VNwdCvTEcuaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = glove_model[\"king\"]\n",
        "word2 = glove_model[\"female\"]\n",
        "\n",
        "result = word2 + word1"
      ],
      "metadata": {
        "id": "mot4_20FcwkK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To retrieve the word from a GloVe vector, you need to find the word whose vector is closest to the given GloVe vector. This is typically done using cosine similarity:"
      ],
      "metadata": {
        "id": "QqsklPmPdRVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model.similar_by_vector(result, topn=5)"
      ],
      "metadata": {
        "id": "P0fCr0dNdnzc",
        "outputId": "587aa7c8-7721-42ed-db70-3899f2f71288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.7884284853935242),\n",
              " ('female', 0.7797942757606506),\n",
              " ('male', 0.7577809691429138),\n",
              " ('queen', 0.7418192028999329),\n",
              " ('father', 0.7039036154747009)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}