{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbGbBAm/G2RxUbQyHnHsRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/nlp_rag/blob/master/langchain_masterclass/03_Chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTK7tlp1qc7B",
        "outputId": "e5185f64-8582-4995-e86f-c580cab5c713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_community langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence, RunnableParallel"
      ],
      "metadata": {
        "id": "y5CQ__6Hq7SM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\".env\"):\n",
        "    os.remove(\".env\")\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    if load_dotenv(\".env\"):\n",
        "        print(\"Uploaded and Loaded Sucessfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "dg40UC0iq47h",
        "outputId": "69f421e0-11c9-4884-a426-130bb697d5e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8bfe1048-9650-4c8c-85dd-293ac08874c5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8bfe1048-9650-4c8c-85dd-293ac08874c5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving .env to .env\n",
            "Uploaded and Loaded Sucessfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load LLM Model"
      ],
      "metadata": {
        "id": "A62umtqwtTDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model='gpt-3.5-turbo-0125')\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC2OigdescDJ",
        "outputId": "42024f7f-89fe-48e0-d972-524c5a46d1d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b875a80c4f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b875a80c730>, root_client=<openai.OpenAI object at 0x7b8759270a00>, root_async_client=<openai.AsyncOpenAI object at 0x7b875a80c5b0>, model_name='gpt-3.5-turbo-0125', model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Chains\n",
        "\n",
        "Chains are sequences of calls that allow you to create complex workflows in LangChain by combining LLMs with other utilities in a specific order. This orchestration helps manage the execution and information flow between components. Chains are essential for building intricate applications with LangChain.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*K_B_09VKpSl9Sgbwd7a7TA.png)"
      ],
      "metadata": {
        "id": "rq_Jf3xxWYZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    messages = [\n",
        "        (\"system\", \"You are a {subject} teacher. Answer only within 50 tokens\"),\n",
        "        (\"human\", \"Tell me what is {topic} ?\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "## Create a combined chain using a LangChain Expression Language (LCEL)\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"subject\": \"physics\", \"topic\": \"Newton 1st law\"})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "z-XKeYnMcGda",
        "outputId": "27ede363-3067-4c72-dc5f-4a989060a798"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Newton's First Law states that an object will remain at rest or in uniform motion unless acted upon by an external force.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LCEL Chains under the hood\n",
        "\n",
        "how chains uses series of `Runnables` under the hood\n",
        "\n",
        "LCEL provides a more user-friendly and declarative way to define chains, but they are ultimately translated into RunnableSequences for execution.\n",
        "\n",
        "Here are some important points to know about this relationship:\n",
        "\n",
        "- RunnableSequence: Forms the foundation for executing chains. It defines a sequence of steps (Runnables) that are executed in order.\n",
        "- Runnable: Represents a single step in a chain. It can be an LLM, a prompt template, an output parser, or any other function.\n",
        "- LCEL: Provides syntactic sugar over RunnableSequences, making it easier to define chains with the | operator.\n",
        "- Under the Hood: When an LCEL chain is invoked, it's converted into a RunnableSequence with the corresponding Runnables.\n",
        "- Benefits: This abstraction makes LangChain chains more modular, flexible, and composable."
      ],
      "metadata": {
        "id": "-56zBO2cihLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
        "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
        "parse_output = RunnableLambda(lambda x: x.content)\n",
        "\n",
        "chain = RunnableSequence(\n",
        "    first = format_prompt,\n",
        "    middle=[invoke_model],\n",
        "    last = parse_output\n",
        ")\n",
        "\n",
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Rfkd9UippC",
        "outputId": "b0725b3d-e7ae-4c3f-e28c-278887c3c296"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
              "| RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
              "| RunnableLambda(lambda x: x.content)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"subject\": \"chemistry\", \"topic\": \"theory of relativity\"})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PFrPI-TplNeG",
        "outputId": "98ba84c8-5110-4a84-80c6-b5ef1082bd69"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The theory of relativity, proposed by Albert Einstein, describes how time, space, and mass are connected. It has two main parts - the general theory of relativity and the special theory of relativity.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extending the chain\n",
        "\n",
        "We can extend the chain with more additional functions like creating some metadata for the output generated by the model like:"
      ],
      "metadata": {
        "id": "OaaInnqinWwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    messages = [\n",
        "        (\"system\", \"You are a {subject} teacher. Answer only within 50 tokens\"),\n",
        "        (\"human\", \"Tell me what is {topic} ?\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "title_case = RunnableLambda(lambda x: x.title())\n",
        "word_count = RunnableLambda(lambda x: f\"content: {x}\\n\\nWord count: {len(x.split(' '))}\")\n",
        "\n",
        "\n",
        "chain_2 = prompt_template | model | StrOutputParser() | title_case | word_count\n",
        "result = chain_2.invoke({\"subject\": \"english\", \"topic\": \"shakespear\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGRjWV01nggR",
        "outputId": "7fe2a0c9-1526-4752-994c-222d80810837"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content: Shakespeare Is Regarded As One Of The Greatest Playwrights And Poets In English Literature. His Works Include Famous Plays Such As \"Romeo And Juliet,\" \"Hamlet,\" And \"Macbeth.\"\n",
            "\n",
            "Word count: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of Chains\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*f6p_BgKsmGOUGUV2pagGFg.png)"
      ],
      "metadata": {
        "id": "XTo3GAfVp9ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Chains in Parallel\n",
        "\n",
        "Chain:\n",
        "\n",
        "1. Prompt_template -> Product\n",
        "2. model -> list of feature\n",
        "3. StrOutputParser\n",
        "4. RunnableParallel -> Invoke two prompts in parallel\n",
        "5. RunnableLambda -> combine the outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "WVJyYTOJp5bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    messages = [\n",
        "        (\"system\", \"you are a tech product reviewer.\"),\n",
        "        (\"human\", \"Please provide the list of features for {product}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "def analyse_pros_template(features):\n",
        "    prompt_template = ChatPromptTemplate.from_messages(\n",
        "        messages = [\n",
        "            ('system', \"You are an expert tech reviwer\"),\n",
        "            (\"human\", \"Provide the list of pros from the given {features}\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return prompt_template.format_prompt(features=features)\n",
        "\n",
        "def analyse_cons_template(features):\n",
        "    prompt_template = ChatPromptTemplate.from_messages(\n",
        "        messages = [\n",
        "            ('system', \"You are an expert tech reviwer\"),\n",
        "            (\"human\", \"Provide the list of cons from the given {features}\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return prompt_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "run_pros_chain = RunnableLambda(lambda x: analyse_pros_template(x)) | model | StrOutputParser()\n",
        "\n",
        "run_cons_chain = RunnableLambda(lambda x: analyse_cons_template(x)) | model | StrOutputParser()\n",
        "\n",
        "final_output = RunnableLambda(lambda x: print(\"Pros:\\n\",x[\"branches\"]['pros'],\"\\n\\nCons:\\n\",x['branches']['cons']))"
      ],
      "metadata": {
        "id": "siDTXxdavOlh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_review_chain = (prompt_template\n",
        "                        | model\n",
        "                        | StrOutputParser()\n",
        "                        | RunnableParallel(branches = {\"pros\": run_pros_chain, \"cons\": run_cons_chain})\n",
        "                        | final_output\n",
        ")"
      ],
      "metadata": {
        "id": "pm52ZkVFuUNN"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_review_chain.invoke({'product': \"Apple Macbook\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSRfrbZrvOCJ",
        "outputId": "6d20ea48-42c5-455e-fb59-eca37d825fcf"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros:\n",
            " Thank you for providing the details about the Apple MacBook. Below is the list of pros based on the features you shared:\n",
            "\n",
            "1. **Retina Display**: The high-resolution display with vibrant colors and sharp details provides an excellent viewing experience for tasks like graphic design, photo editing, and watching videos.\n",
            "\n",
            "2. **Touch Bar and Touch ID**: The Touch Bar offers customizable shortcuts that can enhance productivity, and the Touch ID adds a convenient and secure method for logging in and authorizing purchases.\n",
            "\n",
            "3. **Powerful Processor**: The latest Intel processors ensure fast performance and smooth multitasking, making it suitable for demanding tasks like video editing and 3D rendering.\n",
            "\n",
            "4. **macOS Operating System**: The macOS operating system is known for its stability, ease of use, and seamless integration with other Apple devices, providing a cohesive ecosystem for users.\n",
            "\n",
            "5. **Thin and Lightweight Design**: The sleek and portable design makes it easy to carry around, suitable for users who are always on-the-go or need to work from different locations.\n",
            "\n",
            "6. **High-quality Build**: The premium aluminum construction not only adds durability but also gives the MacBook a sophisticated look and feel, appealing to users who value aesthetics.\n",
            "\n",
            "7. **Thunderbolt 3 Ports**: The Thunderbolt 3 ports offer high-speed data transfer rates and versatile connectivity options, allowing users to connect to various peripherals and external displays with ease.\n",
            "\n",
            "8. **Long Battery Life**: The all-day battery life ensures that users can work or entertain themselves for extended periods without constantly needing to recharge, promoting productivity and convenience.\n",
            "\n",
            "9. **Solid-State Drive (SSD) Storage**: The fast and efficient SSD storage significantly improves the overall system performance by reducing boot-up times and enhancing app loading speeds.\n",
            "\n",
            "10. **Advanced Security Features**: The built-in security features such as Secure Boot, encryption, and Touch ID ensure that user data remains protected, providing peace of mind for users who prioritize data security.\n",
            "\n",
            "Overall, the Apple MacBook offers a combination of premium design, powerful performance, and user-friendly features, making it a popular choice for professionals, creatives, and tech enthusiasts. \n",
            "\n",
            "Cons:\n",
            " While the Apple MacBook offers many impressive features, there are some potential drawbacks to consider. Here are some cons:\n",
            "\n",
            "1. **High Price**: Apple products, including MacBooks, are known for their premium pricing, which may not be affordable for all users.\n",
            "\n",
            "2. **Limited Customization**: MacBooks have limited hardware customization options compared to some Windows laptops, which can be a disadvantage for users who want specific hardware configurations.\n",
            "\n",
            "3. **Butterfly Keyboard**: Some users have reported issues with the butterfly keyboard mechanism on recent MacBook models, which can lead to key sticking or failure.\n",
            "\n",
            "4. **Limited Gaming Options**: While MacBooks can handle casual gaming and productivity tasks well, they may not be the best choice for demanding gaming due to limited game library and performance compared to dedicated gaming laptops.\n",
            "\n",
            "5. **Limited Ports**: The newer MacBook models come with minimal ports, mainly relying on USB-C/Thunderbolt 3 ports, which can require additional adapters for connecting legacy devices.\n",
            "\n",
            "6. **Upgradability**: MacBooks are not known for being user-upgradeable, so storage and memory upgrades may be limited or costly.\n",
            "\n",
            "7. **Dependency on Apple Ecosystem**: To fully utilize the MacBook, users may need to invest in other Apple products/services, which can create a vendor lock-in situation.\n",
            "\n",
            "8. **Not Ideal for Every Professional Use Case**: While MacBooks are highly capable machines, they may not be optimized for certain professional applications or industries that require specific software compatibility or features.\n",
            "\n",
            "Despite these drawbacks, the Apple MacBook lineup remains popular for its blend of design, performance, and user experience. It's essential to consider these cons alongside the benefits when making a purchase decision.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "**Parallel Execution:**\n",
        "- **Efficiency:** Faster as tasks run simultaneously.\n",
        "- **Resource Utilization:** Better use of multiple cores/threads.\n",
        "- **Complexity:** More complex due to synchronization needs.\n",
        "\n",
        "**Sequential Execution:**\n",
        "- **Simplicity:** Easier to implement and debug.\n",
        "- **Predictability:** More predictable flow.\n",
        "- **Time-Consuming:** Slower as tasks run one after another.\n",
        "\n",
        "**LangChain on a Single Machine:**\n",
        "- **Multiple Cores:** Distributes tasks across cores for true parallelism.\n",
        "- **Multithreading:** Uses threads on a single core for concurrent execution, improving performance for I/O-bound tasks.\n",
        "\n",
        "**Local LLM Models:**\n",
        "- **Resource Utilization:** Fully leverages local hardware resources.\n",
        "- **Latency Reduction:** Reduces network latency, enhancing parallel execution efficiency.\n",
        "- **Customization:** Allows for environment optimizations tailored to specific hardware and use cases."
      ],
      "metadata": {
        "id": "ikLqINJ3z2Wa"
      }
    }
  ]
}