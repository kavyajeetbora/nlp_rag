{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHTRd/+0hCt3Mzi2XjIcmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/nlp_rag/blob/master/end_to_end/04_evaluating_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "H9eaYFuVJJ0g"
      },
      "outputs": [],
      "source": [
        "!pip install -q ragas sacrebleu rouge_score rapidfuzz langchain openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import BleuScore, RougeScore\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xGM32aCaJPTf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traditional non LLM Metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "qJhf_9_AMLiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non LLM String Simiarity\n",
        "\n",
        "Non LLM String Similarity:\n",
        "- Metric That measures the distance between generated and reference text using distance measures such as Levenshtein, Hamming and Jaro.\n",
        "- The metric doesnot rely on LLM models but it is heuristic based method.\n",
        "- The metric score is between 0 to 1\n",
        "\n",
        "How **NonLLMStringSimilarity** is calculated with examples for the three common metrics we discussed:\n",
        "\n",
        "### 1. Levenshtein Distance\n",
        "**Calculation**: Measures the minimum number of single-character edits needed to transform one string into another (insertions, deletions, or substitutions).\n",
        "\n",
        "**Example**:\n",
        "- **String 1**: \"kitten\"\n",
        "- **String 2**: \"sitting\"\n",
        "- **Levenshtein Distance**: 3 (replace 'k' with 's', replace 'e' with 'i', insert 'g')\n",
        "\n",
        "### 2. Hamming Distance\n",
        "**Calculation**: Measures the number of positions at which the corresponding characters in two strings differ. Applicable to strings of the same length.\n",
        "\n",
        "**Example**:\n",
        "- **String 1**: \"karolin\"\n",
        "- **String 2**: \"kathrin\"\n",
        "- **Hamming Distance**: 3 (positions 3, 4, and 6 are different)\n",
        "\n",
        "### 3. Jaro-Winkler Distance\n",
        "**Calculation**: Measures the similarity between two strings by considering the number and order of matching characters, with adjustments for common prefixes.\n",
        "\n",
        "**Example**:\n",
        "- **String 1**: \"martha\"\n",
        "- **String 2**: \"marhta\"\n",
        "- **Jaro-Winkler Distance**: Higher similarity score compared to \"martha\" and \"marie\" because \"martha\" and \"marhta\" have more characters in common and in the same order.\n",
        "\n",
        "### Summary\n",
        "- **Levenshtein Distance**: Focuses on the number of edits required to make two strings identical. Lower value = more similar.\n",
        "- **Hamming Distance**: Focuses on character differences in strings of equal length. Lower value = more similar.\n",
        "- **Jaro-Winkler Distance**: Focuses on matching characters and their order, with higher values indicating greater similarity.\n",
        "\n",
        "These traditional string similarity measures help determine how close two strings are based on their character content, rather than semantic meaning."
      ],
      "metadata": {
        "id": "UDo3xqrAU_W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics._string import NonLLMStringSimilarity, DistanceMeasure"
      ],
      "metadata": {
        "id": "2KYbkLWEUdk1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = SingleTurnSample(\n",
        "    response = \"The Eiffel Tower is located in India\",\n",
        "    reference = \"The Eiffel Tower is located in Paris\"\n",
        ")"
      ],
      "metadata": {
        "id": "k38jYdYZVYl5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = NonLLMStringSimilarity()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuadTawTViU9",
        "outputId": "3cba0b45-0692-485d-fc7d-ab4af43e8cc5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8888888888888888"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance based:"
      ],
      "metadata": {
        "id": "vA_zO4y8V3z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = NonLLMStringSimilarity(distance_measure=DistanceMeasure.HAMMING)\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vOKVaxBV24z",
        "outputId": "6a53bcdc-2993-4528-aeb7-1b70d1d2b5c9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8888888888888888"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU Score\n",
        "\n",
        "\n",
        "- The BLUE score compares the response with the reference text based on the n-gram precision and brevity penalty.\n",
        "- It was originally created for machine translation systems, but also used for NLP task\n",
        "- BLEU Score ranges from 0 to 1\n",
        "- Non LLM Metric\n",
        "- It is also case sensitive"
      ],
      "metadata": {
        "id": "IHpEdcs9WIJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import BleuScore\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"The Eiffel Tower is located in India.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\"\n",
        ")\n",
        "\n",
        "scorer = BleuScore()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBLcNtXUWkbK",
        "outputId": "57b7535b-38ff-4a57-cbb3-9e8b3c3b96b4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7071067811865478"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE Score\n",
        "\n",
        "- This score is primarily used for evaluating the quality of NLP text generations.\n",
        "- It measures the overlaps between the response and the reference based on n-gram recall, precision and F1-score.\n",
        "- It ranges from 0-1\n",
        "\n",
        "\n",
        "There are four types of ROUGE score:\n",
        "1. ROUGE-N: measures the n-gram overlaps. For example ROUGE-1 measures unigrams (single words) and ROUGE-2 measures the bigrams (two word sequences)\n",
        "\n",
        "2. ROUGE-L:\n",
        "- Measures longest matching sequence of words without requiring them to be consecutive.\n",
        "- useful for capturing sentence structure and fluency.\n",
        "\n",
        "3. ROUGE-W (Weight LCS): Similar to ROUGE-L but assigns higher weight to longer consecutive matches.\n",
        "\n",
        "4. ROUGE-S (Skip-Bigrams): Measures overlap of pairs of words that appear in the same order in both summaries, allowing for gaps between them irrespective to the position of these bigrams.\n",
        "\n",
        "\n",
        "How does ROUGE calculate the Precision, Recall?\n",
        "\n",
        "1. Recall (R) = ovelapping n-grams / total n-grams in reference\n",
        "\n",
        "High recall means the summary includes most of the important reference words.\n",
        "\n",
        "2. Precision (P) = ovelappingn-grams / total n-grams in generated text\n",
        "\n",
        "High precision means the summary is concise and contains less unnecessary content\n",
        "\n",
        "3. F1 score: which is the harmonic mean of precision and recall.\n"
      ],
      "metadata": {
        "id": "DIUQ7o1lW2_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import RougeScore\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"The Eiffel Tower is located in India.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\"\n",
        ")\n",
        "\n",
        "scorer = RougeScore()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grVa0yfFc5BM",
        "outputId": "bc66eeba-f4b6-4a5e-d974-5da2d93926b3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exact Match\n",
        "\n",
        "It checks whether the generated text is exact match of the reference. It will compare word to word\n",
        "\n",
        "- Either `1` if it is an Exact Match\n",
        "- Else `0`"
      ],
      "metadata": {
        "id": "N2mIGg2OdD8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import ExactMatch\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"India\",\n",
        "    reference=\"Paris\"\n",
        ")\n",
        "\n",
        "scorer = ExactMatch()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFWlYq79eapR",
        "outputId": "dde096d6-4536-4e74-85ac-c0b3de8659f3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = SingleTurnSample(\n",
        "    response=\"Paris\",\n",
        "    reference=\"Paris\"\n",
        ")\n",
        "\n",
        "scorer = ExactMatch()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqJ8TUfOete9",
        "outputId": "2842456c-3b2c-4e9e-d940-75bb77a459d0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### String Presence\n",
        "\n",
        "Metric checks if the response contains the reference text. It is useful to check whether certain keywords is present in the generated text.\n",
        "\n",
        "- Can be useful for data extraction\n",
        "- Remember it is a case sensitive"
      ],
      "metadata": {
        "id": "y-r3UllCe9U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import StringPresence\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    response=\"This is not Spam mail\",\n",
        "    reference=\"spam\"\n",
        ")\n",
        "\n",
        "scorer = StringPresence()\n",
        "await scorer.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtlODy7Oe_Hf",
        "outputId": "3e85be2d-2fd9-49f2-d9f2-d23d34506f82"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Text Summarization"
      ],
      "metadata": {
        "id": "gqmdY6w1LMf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_test_data = {\n",
        "    \"user_input\": \"summarise given text\\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.\",\n",
        "    \"response\": \"The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.\",\n",
        "    \"reference\": \"The company reported an 8% growth in Q3 2024, primarily driven by strong sales in the Asian market, attributed to strategic marketing and localized products, with continued growth anticipated in the next quarter.\"\n",
        "}\n",
        "test_sample_data = SingleTurnSample(**summary_test_data)"
      ],
      "metadata": {
        "id": "IQSIM5OpK0Xi"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = BleuScore()\n",
        "metric.single_turn_score(test_sample_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnozSaG9Jzzy",
        "outputId": "8a74b72d-d247-43b5-e0f6-96a581a7e9ba"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13718598426177148"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_metric = RougeScore()\n",
        "rouge_metric.single_turn_score(test_sample_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poANcFbULfjA",
        "outputId": "31d633ad-341b-49c5-e08b-ecd8be2841f2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating RAG Pipeline\n",
        "\n",
        "Now we can evaluate our pipeline into 2 parts\n",
        "\n",
        "1. evaluating the retrieval part\n",
        "    - Context Precision\n",
        "    - Context Recall\n",
        "2. evaluating the generation part\n",
        "    - Answer Relevance\n",
        "    - Faithfulness - factual or not/ is hallucinating\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JjDzpyH8jt2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context Precision\n",
        "\n",
        "In a RAG pipeline, during retrieval we get say 5 documents, we will calculate the relevance of each retrieved documents with the query.\n",
        "\n",
        "In RAGAS, the context precision metric requires LLM as a judge to find the relevance of the retrieved text with the user-query\n",
        "\n",
        "To estimate if a retrieved context is relevant or not, this method uses the LLM to compare each of the:\n",
        "\n",
        "`retrieved context or chunk present in retrieved_contexts` with `response`."
      ],
      "metadata": {
        "id": "UrhU3lvNkX5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ],
      "metadata": {
        "id": "SYSecDu6lImF"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\".env\"):\n",
        "    os.remove(\".env\")\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    if load_dotenv(\".env\"):\n",
        "        print(\"Uploaded and Loaded Sucessfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "zjD1t7SbmWCs",
        "outputId": "859be265-d786-4a3c-b271-5fb9099f5db8"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-879572ed-01ba-4eab-8ce3-8f61e1f849d1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-879572ed-01ba-4eab-8ce3-8f61e1f849d1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving .env to .env\n",
            "Uploaded and Loaded Sucessfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "evaluator_llm = LangchainLLMWrapper(chat_model)"
      ],
      "metadata": {
        "id": "RELbRrTll7jy"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
        "\n",
        "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input=\"Where is the Eiffel Tower located?\",\n",
        "    response=\"The Eiffel Tower is located in Paris.\",\n",
        "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"],\n",
        ")\n",
        "\n",
        "\n",
        "await context_precision.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMmapQ86k1rm",
        "outputId": "df7a9865-9190-466c-f9a3-e1abd092d1ba"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import LLMContextPrecisionWithReference\n",
        "\n",
        "context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input=\"Where is the Eiffel Tower located?\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\",\n",
        "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"],\n",
        ")\n",
        "\n",
        "await context_precision.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_twBYYMmnEAh",
        "outputId": "edd0004e-af09-473a-9564-57bb0dcb50ee"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import NonLLMContextPrecisionWithReference\n",
        "\n",
        "context_precision = NonLLMContextPrecisionWithReference()\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"],\n",
        "    reference_contexts=[\"Paris is the capital of France.\", \"The Eiffel Tower is one of the most famous landmarks in Paris.\"]\n",
        ")\n",
        "\n",
        "await context_precision.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyuD48IinRNQ",
        "outputId": "af3a3012-9961-44ea-9a73-09c61f75cbc4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTScore\n",
        "\n",
        "### What is BERTScore?\n",
        "- **BERTScore** is a token-based similarity evaluation metric.\n",
        "- It leverages a large language model (LLM) like BERT to measure the similarity between two texts.\n",
        "\n",
        "### How BERTScore Works\n",
        "1. **Tokenization**: The texts are tokenized into smaller units (tokens), usually words or subwords.\n",
        "2. **Embedding**: Each token is converted into a high-dimensional vector (embedding) using a pre-trained BERT model.\n",
        "3. **Similarity Calculation**: Cosine similarity is used to compare the embeddings of tokens from the two texts.\n",
        "4. **Aggregation**: Token similarities are aggregated to compute an overall similarity score. This involves precision, recall, and F1-score calculations.\n",
        "\n",
        "### Example\n",
        "```python\n",
        "from bert_score import score\n",
        "\n",
        "candidate = [\"The cat sat on the mat.\"]\n",
        "references = [\"The cat was on the mat.\"]\n",
        "\n",
        "P, R, F1 = score(candidate, references, lang='en', model_type='bert-base-uncased')\n",
        "print(f\"Precision: {P}\\nRecall: {R}\\nF1: {F1}\")\n",
        "```\n",
        "\n",
        "### When to Use BERTScore\n",
        "1. Text Summarization: To evaluate the quality of generated summaries by comparing them with reference summaries.\n",
        "2. Machine Translation: To assess the accuracy and relevance of translated text compared to reference translations.\n",
        "3. Paraphrasing: To measure how closely a paraphrased text matches the original text in meaning.\n",
        "4. Text Generation: To evaluate the semantic similarity between generated text and reference text in various natural language generation tasks.\n",
        "\n",
        "### Advantages of BERTScore\n",
        "1. Captures semantic similarity by considering contextual meaning.\n",
        "2. Provides a more nuanced evaluation compared to traditional string-based metrics.\n",
        "3. Effective for evaluating tasks where meaning and context are important.\n"
      ],
      "metadata": {
        "id": "FGIt0zJ1seXM"
      }
    }
  ]
}